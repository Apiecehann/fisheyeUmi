<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="An empirical study on the properties of wrist-mounted fisheye cameras for robotic imitation learning.">
  <meta name="keywords" content="Fisheye, UMI, Manipulation, Robotic Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Rethinking Camera Choice : An Empirical Study on Fisheye Camera Properties in Robotic Manipulation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Rethinking Camera Choice: An Empirical Study on Fisheye Camera Properties in Robotic Manipulation</h1>
          <div class="is-size-4 publication-authors">
            <span class="author-block has-text-weight-bold">CVPR 2026</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://hanxue.me/">Han Xue</a><sup>1*</sup>,</span>
            <span class="author-block"><a href="https://apiecehann.github.io/">Min Nan</a><sup>2*</sup>,</span>
            <span class="author-block"><a href="#">Xiaotong Liu</a><sup>3,4*</sup>,</span>
            <span class="author-block"><a href="https://wendichen.me/">Wendi Chen</a><sup>1,4</sup>,</span>
            <span class="author-block"><a href="#">Yuan Fang</a><sup>1,4</sup>,</span>
            <span class="author-block"><a href="#">Jun Lv</a><sup>5</sup>,</span>
            <span class="author-block"><a href="https://www.mvig.org/">Cewu Lu</a><sup>1,4,5‡</sup>,</span>
            <span class="author-block"><a href="#">Chuan Wen</a><sup>1‡</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Southeast University,</span>
            <span class="author-block"><sup>3</sup>USTC,</span>
            <span class="author-block"><sup>4</sup>Shanghai Innovation Institute,</span>
            <span class="author-block"><sup>5</sup>Noematrix Ltd.</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">* Equal contribution.</span>
            <span class="author-block">‡ Equal advising.</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/Demos.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        A systematic study revealing how <strong>wide FoV fisheye cameras</strong> enhance robotic manipulation through superior spatial localization and scene generalization.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The adoption of fisheye cameras in robotic manipulation, driven by their exceptionally wide Field of View (FoV), is rapidly outpacing a systematic understanding of their downstream effects on policy learning. This paper presents the first comprehensive empirical study to bridge this gap, rigorously analyzing the properties of wrist-mounted fisheye cameras for imitation learning. Through extensive experiments in both simulation and the real world, we investigate three critical research questions: spatial localization, scene generalization, and hardware generalization. Our investigation reveals that:
          </p>
          <ul>
            <li><strong>Spatial Localization:</strong> Wide FoV significantly enhances localization, but this benefit is critically contingent on the <strong>visual complexity</strong> of the environment.</li>
            <li><strong>Scene Generalization:</strong> Fisheye-trained policies unlock <strong>superior scene generalization</strong> when trained with sufficient environmental diversity.</li>
            <li><strong>Hardware Generalization:</strong> We identify <strong>scale overfitting</strong> as the root cause of transfer failures and propose <strong>Random Scale Augmentation (RSA)</strong> to improve performance.</li>
          </ul>
          <p>
            Collectively, our findings provide concrete, actionable guidance for the large-scale collection and effective use of fisheye datasets in robotic learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experimental Setup</h2>
    
    <div class="columns is-centered">
      <div class="column is-12">
        <h3 class="title is-4 has-text-centered">Real-World Platform</h3>
        <div class="content has-text-centered">
          <img src="./static/images/figure2.png" alt="Real-World Hardware Setup" style="width: 100%;">
          <p class="has-text-justified mt-3">
            <strong>Figure 1</strong>: Our hardware platform comprises a <strong>Flexiv Rizon 4</strong> 7-axis robot arm and a <strong>DH AG-160-95</strong> adaptive gripper. High-quality demonstrations are collected via teleoperation with a <strong>Meta Quest 3</strong> headset.
          </p>
        </div>
      </div>
    </div> 

    <div class="columns is-centered">
      <div class="column is-12">
        <h3 class="title is-4 has-text-centered">Simulation Environment</h3>
        <div class="content has-text-centered">
          <img src="./static/images/figure3.png" alt="Simulation Rendering Pipeline" style="width: 100%;">
          <p class="has-text-justified mt-3">
            <strong>Figure 2</strong>: To enable reliable fisheye benchmarking, we implemented a <strong>two-stage projection pipeline</strong> within the <strong>MuJoCo</strong> physics engine. This process generates fisheye views from intermediate panoramic representations, allowing for precise lens parameter control.
          </p>
        </div>
      </div>
    </div> 

    <div class="columns is-centered">
      <div class="column is-12">
        <div class="box has-background-light">
          <h4 class="title is-5">Scaling Simulation Capabilities</h4>
          <p class="content">
            We adapt <strong>Robomimic</strong> and <strong>MimicGen</strong> benchmarks to evaluate our policies across <strong>6 challenging manipulation tasks</strong>. To support RQ2, we utilize <strong>32 distinct background textures</strong> to provide sufficient environmental diversity during training.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Research Questions</h2>
    <h3 class="subtitle is-5 has-text-weight-bold">Overview</h3>
    <div class="content has-text-centered">
      <img src="./static/images/figure1.png" alt="Study Design Overview" style="width: 100%;">
      <p class="has-text-justified mt-4">
        <strong>Figure 3</strong>: Overview of the four factors analyzed to address our research questions: (a) Camera Model; (b) Scene Complexity; (c) Scene Diversity; (d) Camera Parameters.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">RQ1: Spatial Localization</h2>
    <h3 class="subtitle is-5 has-text-weight-bold">Can the wider FoV of fisheye cameras help policy localization?</h3>
    <div class="content has-text-justified">
      <p>
        In this section, we investigate whether the wider FoV of fisheye cameras enhances policy localization. Given our wrist-view-only configuration, the policy must rely on background cues for spatial reasoning.
      </p>
      <div class="box has-background-info-light">
        <p>
          <strong>Hypothesis:</strong> The fisheye's wider FoV enables superior policy localization by integrating a greater density of background features, leading to a strong positive dependency on the visual richness of the scene.
        </p>
      </div>
      <p>
        We validate this through task performance in <strong>feature-poor</strong> vs. <strong>feature-rich</strong> backgrounds. The results confirm that rich backgrounds are critical, with the fisheye camera showing an average gain of <strong>+0.39</strong> in the real world.
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-6">
        <h4 class="title is-5 has-text-centered">Simulation Performance (SR)</h4>
        <table class="table is-fullwidth is-striped is-hoverable">
          <thead>
            <tr><th>Camera</th><th>Feature</th><th>Avg. Success Rate</th></tr>
          </thead>
          <tbody>
            <tr><td>Pinhole (Single)</td><td>Poor / Rich</td><td>0.31 / 0.34 (+0.03)</td></tr>
            <tr class="has-background-warning-light"><td><strong>Fisheye (Single)</strong></td><td>Poor / Rich</td><td><strong>0.57 / 0.66 (+0.09)</strong></td></tr>
          </tbody>
        </table>
        <p class="content is-size-6">Table 1: Performance comparison in simulation.</p>
      </div>
      <div class="column is-6">
        <h4 class="title is-5 has-text-centered">Spatial Awareness Probing</h4>
        <table class="table is-fullwidth is-striped is-hoverable">
          <thead>
            <tr><th>Configuration</th><th>Trans. Error (cm) ↓</th><th>Rot. Error (°) ↓</th></tr>
          </thead>
          <tbody>
            <tr><td>Pinhole Poor</td><td>9.80</td><td>9.160</td></tr>
            <tr class="has-background-success-light"><td><strong>Fisheye Rich</strong></td><td><strong>1.73</strong></td><td><strong>1.578</strong></td></tr>
          </tbody>
        </table>
        <p class="content is-size-6">Table 2: Spatial probing errors in <i>Pick Cup</i>.</p>
      </div>
    </div>
    <div class="notification is-light mt-4">
      <p>
        <strong>Conclusion:</strong> To maximize performance, data should be collected in <strong>visually complex and feature-rich environments</strong>.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">RQ2: Scene Generalization</h2>
    <h3 class="subtitle is-5 has-text-weight-bold">How do fisheye cameras affect generalization to novel backgrounds?</h3>
    <div class="content has-text-justified">
      <p>
        In robotic manipulation, the motion of a wrist-mounted camera naturally induces background shifts, which acts as a form of implicit data augmentation. We investigate the <b>Scaling Law</b> of policy generalization by increasing the number of unique training scenes (N) while holding the total data volume fixed to isolate the impact of environmental diversity.
      </p>
      <div class="box has-background-info-light">
        <p>
          <strong>Hypothesis:</strong> Fisheye-trained policies can more effectively utilize scene diversity to improve generalization, exhibiting a steeper performance scaling curve as the number of unique training scenes (N) increases.
        </p>
      </div>
      <p>
        We validate this through zero-shot evaluation on distinct unseen scenes in both simulation and real-world environments. Our results demonstrate that fisheye cameras exhibit significantly greater scaling potential compared to conventional cameras; for instance, the real-world fisheye policy achieves near-perfect scores with just eight diverse training scenes.
      </p>
    </div>

    <div class="columns is-centered">
      <div class="column is-8 has-text-centered">
        <img src="./static/images/figure4.png" alt="Scaling Law Results" style="width: 100%; border-radius: 10px;">
        <p class="mt-2">
          <strong>Figure 4:</strong> Success rate and normalized score vs. the number of training scenes (N).
        </p>
      </div>
    </div>

    <div class="notification is-light mt-4">
      <p>
        <strong>Conclusion:</strong> Maximizing <strong>environmental diversity</strong> during data collection is essential to unlock the full generalization capabilities of fisheye cameras.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">RQ3: Hardware Generalization</h2>
    <h3 class="subtitle is-5 has-text-weight-bold">Can policies maintain performance when deployed on new fisheye lenses?</h3>
    
    <div class="content has-text-justified">
      <p>
        The varied distortion profiles of fisheye lenses make cross-camera transfer a significant challenge. Policies trained on a specific lens often overfit to the absolute pixel scale of objects to determine distance. When deployed on a new lens with different intrinsic parameters, these scales change, causing the policy to miscalculate depth—either undershooting or overshooting the target—leading to catastrophic failures.
      </p>
      
      <div class="box has-background-info-light">
        <p>
          <strong>Hypothesis:</strong> The primary bottleneck for cross-camera transfer is "Scale Overfitting." This can be mitigated by using <strong>Random Scale Augmentation (RSA)</strong> to force the policy to learn relative spatial relationships rather than absolute pixel sizes.
        </p>
      </div>

      <h3 class="title is-4">Random Scale Augmentation (RSA)</h3>
      <p>
        To address scale sensitivity, we introduce <strong>RSA</strong>, a strategy that compels the network to learn scale-invariant features. During training, RSA samples a random scale factor <i>s</i> from a uniform distribution (e.g., 0.7 to 1.3). Scale factors greater than 1.0 effectuate a "zoom-out" effect, where the image is resized down and the surrounding canvas is padded with black. This prevents the network from memorizing absolute sizes and instead teaches relative cues, such as the scale of a target object relative to the robot's gripper.
      </p>

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content has-text-centered">
            <img src="./static/images/figure5.png" alt="Random Scale Augmentation Strategy" style="width: 100%; border-radius: 10px;">
            <p class="mt-2">
              <strong>Figure 5:</strong> Comparison between standard Random Crop Augmentation (fixed scale) and our proposed Random Scale Augmentation (RSA).
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-10">
        <h4 class="title is-5 has-text-centered">Zero-shot Cross-Camera Success Rate (Simulation)</h4>
        <table class="table is-fullwidth is-striped is-hoverable is-bordered">
          <thead>
            <tr class="has-background-light">
              <th>Evaluation Setting</th>
              <th>Lens Characteristic</th>
              <th>Baseline (Standard Aug.)</th>
              <th><strong>Ours (RSA)</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Seen Param</strong></td>
              <td>Training Baseline</td>
              <td>0.56</td>
              <td><strong>0.67</strong></td>
            </tr>
            <tr>
              <td><strong>Param 1</strong></td>
              <td>Decreased FoV</td>
              <td>0.30</td>
              <td><strong>0.41</strong></td>
            </tr>
            <tr>
              <td><strong>Param 2</strong></td>
              <td>Alternative Projection</td>
              <td>0.41</td>
              <td><strong>0.43</strong></td>
            </tr>
            <tr>
              <td><strong>Param 3</strong></td>
              <td>Geometric Scale Shift</td>
              <td>0.15</td>
              <td><strong>0.57</strong></td>
            </tr>
            <tr>
              <td><strong>Param 4</strong></td>
              <td>Increased Distortion</td>
              <td>0.17</td>
              <td><strong>0.40</strong></td>
            </tr>
            <tr>
              <td><strong>Param 5</strong></td>
              <td>Extreme Focal Length</td>
              <td>0.01</td>
              <td><strong>0.06</strong></td>
            </tr>
          </tbody>
        </table>
        <p class="content is-size-6 has-text-centered">Table 3: Comparison of success rates across six tasks in simulation for unseen camera parameters.</p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-10">
        <h4 class="title is-5 has-text-centered">Real-World Hardware Generalization (Normalized Score)</h4>
        <table class="table is-fullwidth is-striped is-hoverable is-bordered">
          <thead>
            <tr class="has-background-light">
              <th>Physical Lens</th>
              <th>FOV Angle</th>
              <th>Induced Scale Shift</th>
              <th>Baseline Score</th>
              <th><strong>Ours (RSA) Score</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Seen Camera</td>
              <td>180°</td>
              <td>1.0x (Seen)</td>
              <td>1.0000</td>
              <td><strong>1.0000</strong></td>
            </tr>
            <tr>
              <td><strong>Narrow Lens</strong></td>
              <td>150°</td>
              <td>~1.2x (Zoom In)</td>
              <td>0.5000</td>
              <td><strong>0.9500</strong></td>
            </tr>
            <tr>
              <td><strong>Wide Lens</strong></td>
              <td>220°</td>
              <td>~0.8x (Zoom Out)</td>
              <td>0.0025</td>
              <td><strong>0.6000</strong></td>
            </tr>
          </tbody>
        </table>
        <p class="content is-size-6 has-text-centered">Table 4: Zero-shot cross-camera transfer on physical hardware using distinct lenses.</p>
      </div>
    </div>

    <div class="notification is-light mt-4">
      <p>
        <strong>Conclusion:</strong> Standard fisheye policies are highly sensitive to absolute object scale. Learning relative scale via strong data augmentation like <strong>RSA</strong> is essential to ensure policies are robust to hardware variations and can effectively leverage datasets from diverse lens sources.
      </p>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments</h2>
    <hr>
    
    <div class="content">
      <h3 class="title is-4">Main Results</h3>
      <p>
        We employ a <strong>normalized, multi-stage scoring metric</strong> for real-world evaluation to provide a more granular assessment of policy capability than binary success rates. Each evaluation setup consists of <strong>20 trials</strong>, and we report the cumulative normalized score. </p>

      <h4 class="title is-5">Task 1: Pick Cup</h4>
      <div class="columns is-vcentered">
        <div class="column is-7">
          <ul>
            <li><strong>Task Description:</strong> The goal is to pick up a cup from a random starting position and place it upright onto a designated coaster.</li>
            <li><strong>Evaluation Protocol:</strong> We test the policy under two environmental settings: <b>Feature-Poor</b> (solid-colored background) and <b>Feature-Rich</b> (patterned cloths with diverse textures).</li>
            <li><strong>Score Metric:</strong> 
              <ul>
                <li>Stage 1 (0.00 pts): Failed to grasp or place the cup. </li>
                <li>Stage 2 (0.50 pts): Placed on coaster but toppled over. </li>
                <li>Stage 3 (1.00 pts): Successfully placed upright on the coaster. </li>
              </ul>
            </li>
          </ul>
        </div>
        <div class="column is-5">
          <img src="./static/images/task_pick_cup.png" alt="Pick Cup Task" style="border-radius: 8px;">
        </div>
      </div>

      <h4 class="title is-5">Task 2: Fold Towel</h4>
      <div class="columns is-vcentered">
        <div class="column is-7">
          <ul>
            <li><strong>Task Description:</strong> This task requires performing two consecutive folds on a deformable towel. The robot must grasp a corner, fold it diagonally, and then repeat for the second corner.</li>
            <li><strong>Evaluation Protocol:</strong> Focuses on the policy's ability to handle deformable objects and maintain localization across multi-step sequences.</li>
            <li><strong>Score Metric:</strong> Four stages (0.25 pts each) corresponding to: 1. Grasping the first corner, 2. Completing the first fold, 3. Grasping the second corner, and 4. Completing the final fold.</li>
          </ul>
        </div>
        <div class="column is-5">
          <img src="./static/images/task_fold_towel.png" alt="Fold Towel Task" style="border-radius: 8px;">
        </div>
      </div>

      <h4 class="title is-5">Task 3: Hang Chinese Knot</h4>
      <div class="columns is-vcentered">
        <div class="column is-7">
          <ul>
            <li><strong>Task Description:</strong> Requires precise rotational manipulation to hang a Chinese knot onto a designated hook on a stand.</li>
            <li><strong>Evaluation Protocol:</strong> Since the initial grasping phase is successfully completed by most baselines, we focus solely on the precise placement required to secure the knot.</li>
            <li><strong>Score Metric:</strong> 
              <ul>
                <li>Stage 1 (0.00 pts): Failed to hang (e.g., dropped or missed the hook). </li>
                <li>Stage 2 (1.00 pts): Successfully secured the knot onto the hook. </li>
              </ul>
            </li>
          </ul>
        </div>
        <div class="column is-5">
          <img src="./static/images/task_hang_knot.png" alt="Hang Chinese Knot Task" style="border-radius: 8px;">
        </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">RQ1: Spatial Localization</h2>
    <div class="content has-text-justified">
      <p>
        <strong>Hypothesis:</strong> The exceptionally wide Field of View (FoV) of wrist-mounted fisheye cameras enhances policy localization by integrating a greater density of static environmental features as visual anchors. Consequently, we expect policy performance to exhibit a strong positive dependency on the <b>visual complexity</b> of the training scene.
      </p>
    </div>

    <h3 class="title is-4">Quantitative Results</h3>
    <div class="columns is-centered">
      <div class="column is-10">
        <div class="table-container">
          <table class="table is-fullwidth is-striped is-hoverable is-bordered has-text-centered">
            <thead>
              <tr class="has-background-light">
                <th>Task</th>
                <th>Camera Setup</th>
                <th>Feature-Poor (Plain)</th>
                <th>Feature-Rich (Patterned)</th>
                <th>Performance Gain</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="has-text-weight-bold">Pick Cup</td>
                <td>Fisheye (State-free)</td>
                <td>0.525</td>
                <td><strong>0.800</strong></td>
                <td class="has-text-success">+0.275</td>
              </tr>
              <tr>
                <td class="has-text-weight-bold">Fold Towel</td>
                <td>Fisheye (State-free)</td>
                <td>0.100</td>
                <td><strong>0.700</strong></td>
                <td class="has-text-success">+0.600</td>
              </tr>
              <tr>
                <td class="has-text-weight-bold">Hang Chinese Knot</td>
                <td>Fisheye (State-free)</td>
                <td>0.200</td>
                <td><strong>0.500</strong></td>
                <td class="has-text-success">+0.300</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p class="is-size-7 mt-2">
          *Note: All results are based on <b>state-free policies</b> (without proprioception) to isolate the visual localization capability of the visual encoder.
        </p>
      </div>
    </div>

    <h3 class="title is-4 mt-6">Qualitative Rollouts</h3>
    <p class="content">We demonstrate the robustness of fisheye-based policies across different tasks and environmental settings.</p>

    <h4 class="title is-5">Task 1: Pick Cup</h4>
    <div class="columns is-multiline">
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 5px;">
          <source src="./static/videos/pick_pinhole_poor.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7">Pinhole + Poor Scene<br>(Score: 0.125)</p>
      </div>
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 5px;">
          <source src="./static/videos/pick_fisheye_poor.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7">Fisheye + Poor Scene<br>(Score: 0.525)</p>
      </div>
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border: 2px solid #00d1b2; border-radius: 5px;">
          <source src="./static/videos/pick_fisheye_rich.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7"><strong>Fisheye + Rich Scene<br>(Score: 0.800)</strong></p>
      </div>
    </div>

    <h4 class="title is-5">Task 2: Fold Towel</h4>
    <div class="columns is-multiline">
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 5px;">
          <source src="./static/videos/fold_pinhole_rich.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7">Pinhole + Rich Scene<br>(Score: 0.316)</p>
      </div>
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border: 2px solid #00d1b2; border-radius: 5px;">
          <source src="./static/videos/fold_fisheye_rich.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7"><strong>Fisheye + Rich Scene<br>(Score: 0.700)</strong></p>
      </div>
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 5px;">
          <source src="./static/videos/fold_fisheye_rich_state.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7">Fisheye + Rich + State<br>(Score: 0.917)</p>
      </div>
    </div>

    <h4 class="title is-5">Task 3: Hang Chinese Knot</h4>
    <div class="columns is-multiline">
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 5px;">
          <source src="./static/videos/hang_fisheye_poor.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7">Fisheye + Poor Scene<br>(Score: 0.200)</p>
      </div>
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border: 2px solid #00d1b2; border-radius: 5px;">
          <source src="./static/videos/hang_fisheye_rich.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7"><strong>Fisheye + Rich Scene<br>(Score: 0.500)</strong></p>
      </div>
      <div class="column is-4">
        <video autoplay muted loop playsinline width="100%" style="border-radius: 5px;">
          <source src="./static/videos/hang_fisheye_rich_state.mp4" type="video/mp4">
        </video>
        <p class="has-text-centered is-size-7">Fisheye + Rich + State<br>(Score: 0.700)</p>
      </div>
    </div>

    <div class="notification is-light mt-5">
      <p>
        <strong>Conclusion:</strong> These results confirm that the fisheye camera's wide contextual view implicitly encodes the robot's spatial relationship with the environment. This renders explicit proprioceptive state redundant in feature-rich environments, allowing the policy to rely exclusively on vision for high-precision manipulation.
      </p>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{xue2026rethinking,
  title={Rethinking Camera Choice: An Empirical Study on Fisheye Camera Properties in Robotic Manipulation},
  author={Xue, Han and Nan, Min and Liu, Xiaotong and Chen, Wendi and Fang, Yuan and Lv, Jun and Lu, Cewu and Wen, Chuan},
  journal={CVPR},
  year={2026},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is modified from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies Template</a>. Some website materials are adapted from <a href="https://transic-robot.github.io/">TRANSIC</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>